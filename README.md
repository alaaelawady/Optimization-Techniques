# Optimization-Techniques

- This repo includes optimization techniques using linear regression with


- For this practical work,will have to develop a Python program that is able to implement the accelerated gradient descent methods with
adaptive learning rate <b>(GD, SGD, Batch GD, Mini Batch , Adagrad, RMSProp, Adam ) Algorithms
</b> in order to achieve the linear regression of a set of datapoints.

 - The end is the merging of Adam Optimizer and (Batch - MiniBatch ) to see the effect of merging them on Multidimensional data.


